{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d695032",
   "metadata": {},
   "source": [
    "# GraphRAG Implementation with Neo4j\n",
    "\n",
    "This notebook implements a Graph Retrieval Augmented Generation (GraphRAG) system that combines:\n",
    "1. Vector embeddings for semantic similarity search\n",
    "2. Knowledge graph in Neo4j for relationship-based context enrichment\n",
    "\n",
    "The combination provides more comprehensive and accurate responses than either approach alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff938437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings, load_index_from_storage\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Document\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import spacy\n",
    "import glob\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"API_KEY\")\n",
    "\n",
    "# NEO4J SETUP\n",
    "neo4j_url = \"bolt://localhost:7687\"\n",
    "neo4j_username = \"neo4j\"\n",
    "neo4j_password = \"123123123\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e71d1",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "We'll start by loading the CSV files from the result directory and preparing the data for our GraphRAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d37da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV from result directory\n",
    "def load_csv_data(directory=\"result\"):\n",
    "    csv_files = glob.glob(f'{directory}/*.csv')\n",
    "    print(f\"Found {len(csv_files)} CSV files in {directory}\")\n",
    "    \n",
    "    all_dfs = []\n",
    "    \n",
    "    for csv_file in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8-sig', sep=';')\n",
    "            print(f\"  - {csv_file}: {len(df)} rows\")\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file}: {e}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Combined dataset: {len(combined_df)} rows\")\n",
    "        \n",
    "        combined_df['id'] = [str(uuid.uuid4()) for _ in range(len(combined_df))]\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data loaded.\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_csv_data()\n",
    "\n",
    "# Create documents for vector indexing\n",
    "if df is not None:\n",
    "    documents = [Document(text=row['text'], id_=row['id'], metadata={\"label\": row['label']}) \n",
    "                 for _, row in df.iterrows()]\n",
    "    print(f\"Created {len(documents)} documents for indexing\")\n",
    "else:\n",
    "    documents = []\n",
    "    print(\"No documents created due to data loading issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e332506",
   "metadata": {},
   "source": [
    "## Building Vector Index\n",
    "\n",
    "We'll use LlamaIndex to create a vector index for our documents, which will enable semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8982f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM and embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Configure settings\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Split documents into nodes for better chunking\n",
    "text_splitter = SentenceSplitter.from_defaults(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "# Check if we have documents to process\n",
    "if documents:\n",
    "    # Create vector store index\n",
    "    print(\"Creating vector index...\")\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, \n",
    "        show_progress=True,\n",
    "        transformations=[text_splitter]\n",
    "    )\n",
    "    \n",
    "    # Save the index to disk\n",
    "    print(\"Saving vector index to storage...\")\n",
    "    vector_index.storage_context.persist(persist_dir=\"./storage\")\n",
    "    \n",
    "    # Load the index from storage to verify it works\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    loaded_index = load_index_from_storage(storage_context)\n",
    "    print(\"Vector index successfully loaded\")\n",
    "else:\n",
    "    print(\"Skipping vector index creation due to missing documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633c5b1",
   "metadata": {},
   "source": [
    "## Knowledge Graph Construction with Neo4j\n",
    "\n",
    "Now we'll build a knowledge graph in Neo4j using the entities and relationships extracted from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neo4j connection\n",
    "driver = GraphDatabase.driver(neo4j_url, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Function to clear the database\n",
    "def clear_database():\n",
    "    with driver.session() as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        print(\"Database cleared\")\n",
    "\n",
    "# Create constraints and indexes for better performance\n",
    "def create_constraints():\n",
    "    with driver.session() as session:\n",
    "        # Create constraint on chunks\n",
    "        session.run(\"\"\"\n",
    "        CREATE CONSTRAINT chunk_id IF NOT EXISTS\n",
    "        FOR (c:Chunk) REQUIRE c.id IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create constraint on entities\n",
    "        session.run(\"\"\"\n",
    "        CREATE CONSTRAINT entity_name IF NOT EXISTS\n",
    "        FOR (e:Entity) REQUIRE e.name IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create constraint on categories\n",
    "        session.run(\"\"\"\n",
    "        CREATE CONSTRAINT category_name IF NOT EXISTS\n",
    "        FOR (c:Category) REQUIRE c.name IS UNIQUE\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"Constraints created\")\n",
    "\n",
    "# Function to normalize Vietnamese text\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Vietnamese text by removing diacritics and special characters\"\"\"\n",
    "    # Replace Vietnamese diacritics with basic Latin characters\n",
    "    replacements = {\n",
    "        'àáảãạăằắẳẵặâầấẩẫậ': 'a',\n",
    "        'èéẻẽẹêềếểễệ': 'e',\n",
    "        'ìíỉĩị': 'i',\n",
    "        'òóỏõọôồốổỗộơờớởỡợ': 'o',\n",
    "        'ùúủũụưừứửữự': 'u',\n",
    "        'ỳýỷỹỵ': 'y',\n",
    "        'đ': 'd'\n",
    "    }\n",
    "    \n",
    "    result = text.lower()\n",
    "    for chars, replacement in replacements.items():\n",
    "        for c in chars:\n",
    "            result = result.replace(c, replacement)\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    result = re.sub(r'[^\\w\\s]', '', result)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load Vietnamese spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"vi_core_news_lg\")\n",
    "    print(\"Loaded Vietnamese spaCy model\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Function to extract entities using Vietnamese spaCy model\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    \n",
    "    # Since noun_chunks is not available for Vietnamese,use a simple approach to extract potential noun phrases\n",
    "    words = text.split()\n",
    "    for i in range(len(words)):\n",
    "        # Consider single words and bigrams as potential entities\n",
    "        if i < len(words) - 1:\n",
    "            # Bigram\n",
    "            phrase = f\"{words[i]} {words[i+1]}\"\n",
    "            if len(phrase) > 2:\n",
    "                entities.append(phrase)\n",
    "        \n",
    "        # Single word\n",
    "        if len(words[i]) > 2:\n",
    "            entities.append(words[i])\n",
    "    \n",
    "    # Filter out very short entities and normalize\n",
    "    entities = list(set([e.strip() for e in entities if len(e.strip()) > 2]))\n",
    "    return entities\n",
    "\n",
    "# Function to extract relationships (verbs) between entities using LLM\n",
    "def extract_relationships(text, entities):\n",
    "    if len(entities) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Create a prompt for the LLM to extract relationships\n",
    "    template = \"\"\"\n",
    "    Extract meaningful relationships between entities in the following text. \n",
    "    The text is in Vietnamese about university regulations.\n",
    "    \n",
    "    Text: {text}\n",
    "    \n",
    "    Entities: {entities}\n",
    "    \n",
    "    For each pair of entities that have a clear relationship, identify the relationship verb or phrase.\n",
    "    Format your response as a JSON array of objects with the following structure:\n",
    "    [\n",
    "      {{\n",
    "        \"source\": \"Entity1\",\n",
    "        \"target\": \"Entity2\",\n",
    "        \"relationship\": \"relationship_verb\"\n",
    "      }}\n",
    "    ]\n",
    "    \n",
    "    Only include relationships that are explicitly stated or strongly implied in the text.\n",
    "    If no clear relationships exist, return an empty array.\n",
    "    Do not include any explanations, markdown formatting, or code blocks - just return the raw JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    \n",
    "    chain = (\n",
    "        prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"text\": text,\n",
    "        \"entities\": \", \".join(entities)\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Clean up the response to handle potential markdown formatting\n",
    "        cleaned_response = response\n",
    "        # Remove markdown code blocks if present\n",
    "        if \"```json\" in cleaned_response:\n",
    "            cleaned_response = cleaned_response.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        # Remove any leading/trailing whitespace\n",
    "        cleaned_response = cleaned_response.strip()\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        relationships = json.loads(cleaned_response)\n",
    "        return relationships\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse relationship JSON: {e}\\nResponse: {response[:100]}...\")\n",
    "        return []\n",
    "\n",
    "# Function to populate Neo4j with chunks, entities, and relationships\n",
    "def populate_graph(df):\n",
    "    # Clear existing data and create constraints\n",
    "    clear_database()\n",
    "    create_constraints()\n",
    "    \n",
    "    # Create category nodes\n",
    "    with driver.session() as session:\n",
    "        # Extract unique categories from labels\n",
    "        categories = df['label'].str.replace('__label__', '').unique()\n",
    "        \n",
    "        # Create category nodes\n",
    "        for category in categories:\n",
    "            session.run(\"\"\"\n",
    "            MERGE (c:Category {name: $name})\n",
    "            \"\"\", name=category)\n",
    "        \n",
    "        print(f\"Created {len(categories)} category nodes\")\n",
    "    \n",
    "    # Process each document and add to graph\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Building knowledge graph\"):\n",
    "        text = row['text']\n",
    "        chunk_id = row['id']\n",
    "        category = row['label'].replace('__label__', '')\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = extract_entities(text)\n",
    "        \n",
    "        # Extract relationships\n",
    "        relationships = extract_relationships(text, entities)\n",
    "        # Convert relationships to a readable format for debugging and saving\n",
    "        if relationships:\n",
    "            # Create a dictionary for easier access later\n",
    "            rels_dict = {\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": text,\n",
    "                \"category\": category,\n",
    "                \"relationships\": relationships\n",
    "            }\n",
    "            \n",
    "            # Save to a JSON file for review\n",
    "            if not os.path.exists(\"relationships\"):\n",
    "                os.makedirs(\"relationships\")\n",
    "\n",
    "            with open(f\"relationships/{chunk_id}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(rels_dict, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            # Create chunk node\n",
    "            session.run(\"\"\"\n",
    "            MERGE (c:Chunk {id: $id})\n",
    "            SET c.text = $text,\n",
    "                c.category = $category\n",
    "            \"\"\", id=chunk_id, text=text, category=category)\n",
    "            \n",
    "            # Connect chunk to category\n",
    "            session.run(\"\"\"\n",
    "            MATCH (chunk:Chunk {id: $chunk_id})\n",
    "            MATCH (category:Category {name: $category})\n",
    "            MERGE (chunk)-[:BELONGS_TO]->(category)\n",
    "            \"\"\", chunk_id=chunk_id, category=category)\n",
    "            \n",
    "            # Create entity nodes and connect to chunk\n",
    "            for entity in entities:\n",
    "                session.run(\"\"\"\n",
    "                MERGE (e:Entity {name: $name})\n",
    "                \"\"\", name=entity)\n",
    "                \n",
    "                # Connect entity to chunk\n",
    "                session.run(\"\"\"\n",
    "                MATCH (c:Chunk {id: $chunk_id})\n",
    "                MATCH (e:Entity {name: $entity_name})\n",
    "                MERGE (c)-[:CONTAINS]->(e)\n",
    "                \"\"\", chunk_id=chunk_id, entity_name=entity)\n",
    "            \n",
    "            # Create relationships between entities\n",
    "            for rel in relationships:\n",
    "                source = rel['source']\n",
    "                target = rel['target']\n",
    "                relationship = normalize_text(rel['relationship']).upper().replace(' ', '_')\n",
    "                \n",
    "                if len(source) > 2 and len(target) > 2 and source != target:\n",
    "                    # Create the relationship in Neo4j\n",
    "                    query = f\"\"\"\n",
    "                    MATCH (s:Entity {{name: $source}})\n",
    "                    MATCH (t:Entity {{name: $target}})\n",
    "                    MERGE (s)-[r:{relationship}]->(t)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    try:\n",
    "                        session.run(query, source=source, target=target)\n",
    "                    except Exception as e:\n",
    "                        # If dynamic relationship creation fails, use a generic relationship\n",
    "                        session.run(\"\"\"\n",
    "                        MATCH (s:Entity {name: $source})\n",
    "                        MATCH (t:Entity {name: $target})\n",
    "                        MERGE (s)-[r:RELATED_TO]->(t)\n",
    "                        SET r.type = $relationship\n",
    "                        \"\"\", source=source, target=target, relationship=rel['relationship'])\n",
    "\n",
    "# Build the knowledge graph if we have data\n",
    "if df is not None:\n",
    "    populate_graph(df)\n",
    "    print(\"Knowledge graph construction completed\")\n",
    "else:\n",
    "    print(\"Skipping knowledge graph construction due to missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045ebe5",
   "metadata": {},
   "source": [
    "## Graph Exploration and Visualization\n",
    "\n",
    "Let's explore the knowledge graph to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get graph statistics\n",
    "def get_graph_stats():\n",
    "    with driver.session() as session:\n",
    "        # Count nodes by label\n",
    "        node_counts = session.run(\"\"\"\n",
    "        CALL apoc.meta.stats()\n",
    "        YIELD labels\n",
    "        RETURN labels\n",
    "        \"\"\").single()['labels']\n",
    "        \n",
    "        # Count relationship types\n",
    "        rel_counts = session.run(\"\"\"\n",
    "        CALL apoc.meta.stats()\n",
    "        YIELD relTypes\n",
    "        RETURN relTypes\n",
    "        \"\"\").single()['relTypes']\n",
    "        \n",
    "        print(\"Graph Statistics:\")\n",
    "        print(\"=================\")\n",
    "        \n",
    "        print(\"\\nNode counts:\")\n",
    "        for label, count in node_counts.items():\n",
    "            print(f\"  {label}: {count}\")\n",
    "        \n",
    "        print(\"\\nRelationship counts:\")\n",
    "        for rel, count in rel_counts.items():\n",
    "            print(f\"  {rel}: {count}\")\n",
    "        \n",
    "        # Get top entities by connections - fixed query using COUNT instead of size()\n",
    "        top_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WITH e, COUNT { (e)--() } AS connections\n",
    "        ORDER BY connections DESC\n",
    "        LIMIT 10\n",
    "        RETURN e.name AS entity, connections\n",
    "        \"\"\").values()\n",
    "        \n",
    "        print(\"\\nTop 10 entities by connections:\")\n",
    "        for entity, connections in top_entities:\n",
    "            print(f\"  {entity}: {connections} connections\")\n",
    "\n",
    "# Function to visualize a subgraph around a key entity\n",
    "def visualize_entity_subgraph(entity_name, depth=1):\n",
    "    with driver.session() as session:\n",
    "        # Get subgraph around the entity\n",
    "        result = session.run(\"\"\"\n",
    "        MATCH path = (e:Entity {name: $name})-[*1..2]-(related)\n",
    "        RETURN path\n",
    "        LIMIT 50\n",
    "        \"\"\", name=entity_name).values()\n",
    "        \n",
    "        if not result:\n",
    "            print(f\"No subgraph found for entity: {entity_name}\")\n",
    "            return\n",
    "        \n",
    "        # Convert to NetworkX graph for visualization\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for path in result:\n",
    "            path_obj = path[0]  # Neo4j Path object\n",
    "            \n",
    "            # Add nodes and edges from the path\n",
    "            for node in path_obj.nodes:\n",
    "                # Get node properties\n",
    "                props = dict(node.items())\n",
    "                node_type = list(node.labels)[0]\n",
    "                \n",
    "                # Use name for all node types\n",
    "                if 'name' in props:\n",
    "                    node_name = props['name']\n",
    "                elif 'id' in props:\n",
    "                    node_name = props['id']\n",
    "                else:\n",
    "                    node_name = str(node.id)\n",
    "                \n",
    "                # Add node with its type as an attribute\n",
    "                G.add_node(node_name, type=node_type)\n",
    "            \n",
    "            # Add edges\n",
    "            for rel in path_obj.relationships:\n",
    "                # Get source and target node IDs\n",
    "                start_node = rel.start_node\n",
    "                end_node = rel.end_node\n",
    "                \n",
    "                # Get node names\n",
    "                if 'name' in start_node:\n",
    "                    start_name = start_node['name']\n",
    "                else:\n",
    "                    start_name = start_node['id'] if 'id' in start_node else str(start_node.id)\n",
    "                \n",
    "                if 'name' in end_node:\n",
    "                    end_name = end_node['name']\n",
    "                else:\n",
    "                    end_name = end_node['id'] if 'id' in end_node else str(end_node.id)\n",
    "                \n",
    "                # Add edge with relationship type as attribute\n",
    "                G.add_edge(start_name, end_name, type=rel.type)\n",
    "        \n",
    "        # Visualize the graph\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Define node colors based on type\n",
    "        colors = {\n",
    "            'Entity': 'skyblue',\n",
    "            'Chunk': 'lightgreen',\n",
    "            'Category': 'salmon'\n",
    "        }\n",
    "        \n",
    "        # Get node positions using spring layout\n",
    "        pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
    "        \n",
    "        # Draw nodes\n",
    "        for node_type, color in colors.items():\n",
    "            nodes = [n for n, data in G.nodes(data=True) if data.get('type') == node_type]\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=color, node_size=300, alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(G, pos, width=1, alpha=0.5)\n",
    "        \n",
    "        # Draw labels with smaller font\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "        \n",
    "        plt.title(f\"Subgraph around '{entity_name}'\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return G\n",
    "\n",
    "# Get graph statistics\n",
    "get_graph_stats()\n",
    "\n",
    "# Visualize a subgraph around a key entity - replace with an actual entity from your data\n",
    "visualize_entity_subgraph(\"sinh viên\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd9924",
   "metadata": {},
   "source": [
    "## GraphRAG Implementation\n",
    "\n",
    "Now we'll implement the GraphRAG query system that combines vector retrieval with graph-based context enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd4e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get graph context for a query\n",
    "def get_graph_context(query, limit=5):\n",
    "    # Normalize query terms for better matching\n",
    "    query_terms = query.split()\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # First approach: Find entities mentioned in the query\n",
    "        entities_query = \"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE any(term IN $query_terms WHERE e.name CONTAINS term OR $query_text CONTAINS e.name)\n",
    "        RETURN e.name AS entity\n",
    "        LIMIT $limit\n",
    "        \"\"\"\n",
    "        \n",
    "        entities = session.run(\n",
    "            entities_query, \n",
    "            query_terms=query_terms,\n",
    "            query_text=query,\n",
    "            limit=limit\n",
    "        ).values()\n",
    "        \n",
    "        if not entities:\n",
    "            return \"No relevant graph context found.\"\n",
    "        \n",
    "        # Get context for each entity\n",
    "        context_parts = []\n",
    "        \n",
    "        for entity_tuple in entities:\n",
    "            entity_name = entity_tuple[0]\n",
    "            \n",
    "            # Find chunks containing this entity\n",
    "            chunks_query = \"\"\"\n",
    "            MATCH (e:Entity {name: $entity_name})<-[:CONTAINS]-(c:Chunk)\n",
    "            RETURN c.text AS chunk_text, c.category AS category\n",
    "            LIMIT 2\n",
    "            \"\"\"\n",
    "            \n",
    "            chunks = session.run(chunks_query, entity_name=entity_name).values()\n",
    "            \n",
    "            # Find relationships for this entity\n",
    "            rels_query = \"\"\"\n",
    "            MATCH (e:Entity {name: $entity_name})-[r]-(other:Entity)\n",
    "            RETURN type(r) AS relationship, \n",
    "                   e.name AS source, \n",
    "                   other.name AS target,\n",
    "                   r.type AS rel_type\n",
    "            LIMIT 5\n",
    "            \"\"\"\n",
    "            \n",
    "            relationships = session.run(rels_query, entity_name=entity_name).values()\n",
    "            \n",
    "            # Format entity context\n",
    "            entity_context = f\"Entity: {entity_name}\\n\"\n",
    "            \n",
    "            if relationships:\n",
    "                entity_context += \"Relationships:\\n\"\n",
    "                for rel_type, source, target, rel_name in relationships:\n",
    "                    # Handle both dynamic and generic relationships\n",
    "                    relationship = rel_name if rel_type == \"RELATED_TO\" else rel_type.lower().replace('_', ' ')\n",
    "                    entity_context += f\"  - {source or entity_name} {relationship} {target}\\n\"\n",
    "            \n",
    "            context_parts.append(entity_context)\n",
    "        \n",
    "        # Return combined context\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Function to retrieve chunks using vector search\n",
    "def get_vector_results(query, top_k=5):\n",
    "    # Load the index from storage\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    \n",
    "    # Create a simple vector retriever\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "    \n",
    "    # Retrieve nodes\n",
    "    nodes = retriever.retrieve(query)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, node in enumerate(nodes):\n",
    "        results.append({\n",
    "            \"text\": node.node.text,\n",
    "            \"score\": node.score,\n",
    "            \"id\": node.node.id_,\n",
    "            \"metadata\": node.node.metadata\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to create a chatbot using GraphRAG\n",
    "def create_graphrag_chatbot():\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "    Bạn là trợ lý thông minh của Đại học Cần Thơ. Nhiệm vụ của bạn là trả lời câu hỏi về các quy định, \n",
    "    thủ tục và chính sách của trường một cách chính xác và hữu ích.\n",
    "    \n",
    "    Sử dụng cả thông tin từ tìm kiếm vector và từ đồ thị tri thức để trả lời câu hỏi một cách toàn diện.\n",
    "    \n",
    "    THÔNG TIN TỪ VECTOR:\n",
    "    {vector_context}\n",
    "    \n",
    "    THÔNG TIN TỪ ĐỒ THỊ:\n",
    "    {graph_context}\n",
    "    \n",
    "    Câu hỏi: {query}\n",
    "    \n",
    "    Yêu cầu:\n",
    "    1. Trả lời câu hỏi dựa trên thông tin được cung cấp\n",
    "    2. Nếu không có đủ thông tin, hãy nói rằng bạn không có thông tin đầy đủ\n",
    "    3. Trình bày câu trả lời rõ ràng, dễ hiểu và có cấu trúc\n",
    "    4. Nếu có thông tin mâu thuẫn, hãy nêu rõ các khả năng\n",
    "    \n",
    "    Trả lời:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = (\n",
    "        prompt \n",
    "        | model \n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "# Function to query the GraphRAG system\n",
    "def query_graphrag(query):\n",
    "    # Get vector search results\n",
    "    vector_results = get_vector_results(query, top_k=5)\n",
    "    \n",
    "    # Format vector context\n",
    "    vector_context = \"\\n\\n\".join([\n",
    "        f\"Đoạn {i+1} (Điểm tương đồng: {result['score']:.4f}):\\n{result['text']}\"\n",
    "        for i, result in enumerate(vector_results)\n",
    "    ])\n",
    "    \n",
    "    # Get graph context\n",
    "    graph_context = get_graph_context(query)\n",
    "    \n",
    "    # Create chatbot\n",
    "    chatbot = create_graphrag_chatbot()\n",
    "    \n",
    "    # Generate response\n",
    "    response = chatbot.invoke({\n",
    "        \"query\": query,\n",
    "        \"vector_context\": vector_context,\n",
    "        \"graph_context\": graph_context\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"vector_results\": vector_results,\n",
    "        \"graph_context\": graph_context\n",
    "    }\n",
    "\n",
    "# Test the GraphRAG system with sample queries\n",
    "sample_queries = [\n",
    "    \"Quy định về học phí của trường là gì?\",\n",
    "    \"Làm thế nào để xin học bổng?\",\n",
    "    \"Quy trình xét tốt nghiệp gồm những bước nào?\",\n",
    "    \"Sinh viên cần điều kiện gì để được ở ký túc xá?\",\n",
    "    \"Khi nào sinh viên bị cảnh cáo học vụ?\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = query_graphrag(query)\n",
    "    \n",
    "    print(\"\\nRESPONSE:\")\n",
    "    print(result[\"response\"])\n",
    "    \n",
    "    print(\"\\nTOP VECTOR RESULTS:\")\n",
    "    for i, res in enumerate(result[\"vector_results\"][:3]):\n",
    "        print(f\"\\n--- Result {i+1} (Score: {res['score']:.4f}) ---\")\n",
    "        print(res[\"text\"][:150] + \"...\" if len(res[\"text\"]) > 150 else res[\"text\"])\n",
    "    \n",
    "    print(\"\\nGRAPH CONTEXT:\")\n",
    "    print(result[\"graph_context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f4952",
   "metadata": {},
   "source": [
    "## Interactive ChatBot Interface\n",
    "\n",
    "Let's create an interactive interface for our GraphRAG chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7745a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_graphrag_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive GraphRAG chat session in the notebook.\n",
    "    Enter 'quit', 'exit', or 'q' to end the session.\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"GRAPHRAG CHATBOT ĐẠI HỌC CẦN THƠ\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Hỏi đáp về quy định, thủ tục và chính sách của trường\")\n",
    "    print(\"Nhập 'quit', 'exit', hoặc 'q' để kết thúc\")\n",
    "    print(\"Nhập 'debug' để xem thông tin truy xuất\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    debug_mode = False\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        query = input(\"\\nBạn: \")\n",
    "        \n",
    "        # Check if user wants to exit\n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\nCảm ơn bạn đã sử dụng chatbot. Chúc bạn một ngày tốt lành!\")\n",
    "            break\n",
    "            \n",
    "        # Check if user wants to toggle debug mode\n",
    "        if query.lower() == 'debug':\n",
    "            debug_mode = not debug_mode\n",
    "            print(f\"\\nDebug mode {'enabled' if debug_mode else 'disabled'}\")\n",
    "            continue\n",
    "            \n",
    "        # Process query and get response\n",
    "        try:\n",
    "            result = query_graphrag(query)\n",
    "            \n",
    "            # Display response\n",
    "            print(\"\\nTrợ lý: \" + result[\"response\"])\n",
    "            \n",
    "            # Show debug information if enabled\n",
    "            if debug_mode:\n",
    "                print(\"\\n--- DEBUG INFO ---\")\n",
    "                print(\"\\nTOP VECTOR RESULTS:\")\n",
    "                for i, res in enumerate(result[\"vector_results\"][:3]):\n",
    "                    print(f\"\\n- Result {i+1} (Score: {res['score']:.4f}) -\")\n",
    "                    print(res[\"text\"][:150] + \"...\" if len(res[\"text\"]) > 150 else res[\"text\"])\n",
    "                \n",
    "                print(\"\\nGRAPH CONTEXT:\")\n",
    "                print(result[\"graph_context\"])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nLỗi xử lý: {e}\")\n",
    "\n",
    "# Uncomment to run the interactive chat\n",
    "# interactive_graphrag_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02462754",
   "metadata": {},
   "source": [
    "## Evaluation and Comparison\n",
    "\n",
    "We'll compare the performance of different retrieval methods:\n",
    "1. Vector-only (traditional RAG)\n",
    "2. Graph-only retrieval\n",
    "3. Combined GraphRAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a735e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query with vector-only approach\n",
    "def query_vector_only(query):\n",
    "    # Get vector search results\n",
    "    vector_results = get_vector_results(query, top_k=5)\n",
    "    \n",
    "    # Format vector context\n",
    "    vector_context = \"\\n\\n\".join([\n",
    "        f\"Đoạn {i+1}:\\n{result['text']}\"\n",
    "        for i, result in enumerate(vector_results)\n",
    "    ])\n",
    "    \n",
    "    # Create prompt template\n",
    "    template = \"\"\"\n",
    "    Bạn là trợ lý thông minh của Đại học Cần Thơ. Trả lời câu hỏi dựa trên thông tin sau:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Câu hỏi: {query}\n",
    "    \n",
    "    Trả lời:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Generate response\n",
    "    response = chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"context\": vector_context\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Function to query with graph-only approach\n",
    "def query_graph_only(query):\n",
    "    # Get graph context\n",
    "    graph_context = get_graph_context(query)\n",
    "    \n",
    "    # Create prompt template\n",
    "    template = \"\"\"\n",
    "    Bạn là trợ lý thông minh của Đại học Cần Thơ. Trả lời câu hỏi dựa trên thông tin sau:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Câu hỏi: {query}\n",
    "    \n",
    "    Trả lời:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Generate response\n",
    "    response = chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"context\": graph_context\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Function to evaluate approaches\n",
    "def compare_approaches(queries):\n",
    "    results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"Processing query: {query}\")\n",
    "        \n",
    "        # Get responses from each approach\n",
    "        vector_response = query_vector_only(query)\n",
    "        graph_response = query_graph_only(query)\n",
    "        graphrag_result = query_graphrag(query)\n",
    "        graphrag_response = graphrag_result[\"response\"]\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"vector_only\": vector_response,\n",
    "            \"graph_only\": graph_response,\n",
    "            \"graphrag\": graphrag_response\n",
    "        })\n",
    "    \n",
    "    # Display results in a table\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    comparison_df.to_csv(\"approach_comparison.csv\", index=False)\n",
    "    print(\"Results saved to approach_comparison.csv\")\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Example evaluation queries\n",
    "eval_queries = [\n",
    "    \"Quy định về kỷ luật sinh viên\",\n",
    "    \"Làm thế nào để chuyển ngành học\",\n",
    "    \"Điều kiện để được học bổng khuyến khích học tập\",\n",
    "    \"Thủ tục xin bảo lưu kết quả học tập\",\n",
    "    \"Quy định về chuẩn đầu ra ngoại ngữ\"\n",
    "]\n",
    "\n",
    "# Uncomment to run comparison\n",
    "# comparison_results = compare_approaches(eval_queries)\n",
    "# comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657fe7b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete GraphRAG implementation that combines:\n",
    "\n",
    "1. **Vector Retrieval**: Traditional semantic search using embeddings\n",
    "2. **Knowledge Graph**: Relationship-aware context from Neo4j\n",
    "3. **LLM Integration**: Structured prompting with context from both sources\n",
    "\n",
    "The combined approach provides more comprehensive and accurate responses by leveraging both the semantic understanding of vector search and the structured relationships of a knowledge graph.\n",
    "\n",
    "Next steps could include:\n",
    "- Fine-tuning the prompts for better responses\n",
    "- Adding more relationship types to the knowledge graph\n",
    "- Implementing user feedback mechanisms to improve retrieval\n",
    "- Deploying the system as a web service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
