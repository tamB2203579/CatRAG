{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from multilingual_pdf2text.models.document_model.document import Document as PDFDocument\n",
    "from multilingual_pdf2text.pdf2text import PDF2Text\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aeac69",
   "metadata": {},
   "source": [
    "**Data Preparation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e491e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize labels for classification task\n",
    "labels = [\"Dao_tao\", \"Hoc_tap_ren_luyen\", \"Khen_thuong_ky_luat\", \"Tot_nghiep\", \"KTX\", \"Khac\"]\n",
    "\n",
    "def load_stop_words():\n",
    "    # Load stopwords.txt\n",
    "    with open(\"lib/vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        stop_words = f.read().splitlines()\n",
    "    return stop_words\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert non-uppercase words to lowercase\n",
    "    words = text.split()\n",
    "    processed_words = [word if word.isupper() else word.lower() for word in words]\n",
    "\n",
    "    # Join words back into a string\n",
    "    text = \" \".join(processed_words)\n",
    "\n",
    "    # Remove unwanted special characters (keep letters, numbers, whitespace, , ? . -)\n",
    "    text = re.sub(r'[^\\w\\s,?.-]', '', text)\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def convert_pdf_to_markdown_with_llm(pdf_path, output_path=None):\n",
    "    # Initialize document object with Vietnamese language setting\n",
    "    pdf_document = PDFDocument(document_path=pdf_path, language=\"vie\")\n",
    "    pdf2text = PDF2Text(document=pdf_document)\n",
    "\n",
    "    # Extract text from PDF\n",
    "    extracted_text = \"\"\n",
    "    for words in pdf2text.extract():\n",
    "        extracted_text += words[\"text\"]\n",
    "\n",
    "    # Load prompt template for PDF to Markdown conversion\n",
    "    with open(\"prompt/pdf_to_markdown.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "\n",
    "    # Set up the LLM chain\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # Create processing chain: pass PDF content through the prompt to the model\n",
    "    chain = (\n",
    "        {\"pdf_content\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # Process the extracted text through the LLM chain\n",
    "    markdown_text = chain.invoke(extracted_text)\n",
    "\n",
    "    # Save to file if output path is provided\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).write_text(markdown_text, encoding=\"utf-8\")\n",
    "        print(f\"Markdown saved to {output_path}\")\n",
    "\n",
    "    return markdown_text\n",
    "\n",
    "def chunking(text):\n",
    "    # text_splitter = MarkdownTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "    # return text_splitter.split_text(text)\n",
    "    text_splitter = SemanticChunker(OpenAIEmbeddings(), breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=95, min_chunk_size=100)\n",
    "    return text_splitter.create_documents([text])\n",
    "\n",
    "def labelling():\n",
    "    choice = input()\n",
    "    return \"__label__\" + labels[int(choice)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad3c4d",
   "metadata": {},
   "source": [
    "**PREPARE CSV DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6531639",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(os.listdir(\"content\")):\n",
    "    print()\n",
    "    content = convert_pdf_to_markdown_with_llm(os.path.join(\"content\", file), os.path.join(\"result\", file.replace(\".pdf\", \".md\")))\n",
    "    content = preprocess(content)\n",
    "    chunks = chunking(content)\n",
    "    data = []\n",
    "    label = labelling()\n",
    "    for chunk in chunks:\n",
    "        data.append({\"text\": chunk.page_content, \"label\": label})\n",
    "    df = pd.DataFrame(data)\n",
    "    path = os.path.join(\"result\", file.replace(\".pdf\", \".csv\"))\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db71b45",
   "metadata": {},
   "source": [
    "**PARAPHRASE DATA FOR CLASSIFICATION MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"classed_dataset.xlsx\", sheet_name=\"KTX\")\n",
    "\n",
    "with open(\"prompt/augment.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "category = df['label'].iloc[0]\n",
    "\n",
    "num_variations = input()\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row['text']\n",
    "    \n",
    "    chain_input = {\n",
    "        \"category\": category,\n",
    "        \"num_variations\": num_variations,\n",
    "        \"text\": text\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = chain.invoke(chain_input)\n",
    "        \n",
    "        augmented_texts = json.loads(result)\n",
    "        \n",
    "        augmented_data.append({\n",
    "            \"text\": text,\n",
    "            \"label\": row['label']\n",
    "        })\n",
    "        \n",
    "        for variant in augmented_texts[0][\"variants\"]:\n",
    "            augmented_data.append({\n",
    "                \"text\": variant,\n",
    "                \"label\": row['label']\n",
    "            })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text at index {index}: {e}\")\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "augmented_df.to_excel(\"augmented_dataset_TN.xlsx\", index=False)\n",
    "print(f\"Original dataset size: {len(df)}, Augmented dataset size: {len(augmented_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f619f3",
   "metadata": {},
   "source": [
    "**CONVERT EXCEL FILE TO CSV FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"dataset.xlsx\")\n",
    "df.to_csv(\"dataset.csv\", encoding=\"utf-8-sig\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
